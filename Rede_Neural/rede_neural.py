import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import warnings
warnings.filterwarnings('ignore')

# Configura√ß√£o para reprodutibilidade
SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)

class ConfiguracaoRedeNeural:
    """Configura√ß√µes centralizadas para a Rede Neural."""
    
    # ARQUITETURA DO MODELO
    HIDDEN_LAYERS = [32, 16]           # Camadas ocultas e neur√¥nios
    DROPOUT_RATE = 0.3                # Taxa de dropout
    L2_REGULARIZATION = 0.001           # Regulariza√ß√£o L2
    
    # OTIMIZA√á√ÉO
    LEARNING_RATE = 0.001              # Taxa de aprendizado
    OPTIMIZER = 'adam'                 # Otimizador
    
    # TREINAMENTO
    EPOCHS = 150                       # M√°ximo de √©pocas
    BATCH_SIZE = 32                    # Tamanho do lote
    
    # EARLY STOPPING
    EARLY_STOPPING_PATIENCE = 20       # √âpocas sem melhoria para parar
    REDUCE_LR_PATIENCE = 10            # √âpocas sem melhoria para reduzir LR
    REDUCE_LR_FACTOR = 0.5             # Fator de redu√ß√£o do LR
    MIN_LEARNING_RATE = 1e-6           # LR m√≠nimo
    
    # PR√â-PROCESSAMENTO
    NORMALIZACAO = 'standard'          # Tipo de normaliza√ß√£o
    
    # DIVIS√ÉO DOS DADOS
    TEST_SIZE = 0.3                    # Propor√ß√£o para teste
    VALIDATION_SIZE = 0.2              # Propor√ß√£o para valida√ß√£o
    
    # VALIDA√á√ÉO CRUZADA
    CV_FOLDS = 5                       # N√∫mero de folds
    CV_EPOCHS = 50                     # √âpocas para cada fold
    
    # IMPORT√ÇNCIA DAS FEATURES
    PERMUTATION_REPEATS = 10           # Repeti√ß√µes para c√°lculo de import√¢ncia
    
    # TESTE DE ROBUSTEZ
    NOISE_LEVELS = [0.1, 0.2, 0.3]    # N√≠veis de ru√≠do gaussiano
    
    @classmethod
    def imprimir_configuracoes(cls):
        """Imprime todas as configura√ß√µes atuais."""
        print("\n" + "="*70)
        print("CONFIGURA√á√ïES ATUAIS DA REDE NEURAL")
        print("="*70)
        
        print(f"\nüèóÔ∏è  ARQUITETURA:")
        print(f"   Camadas Ocultas: {cls.HIDDEN_LAYERS}")
        print(f"   Taxa de Dropout: {cls.DROPOUT_RATE}")
        print(f"   Regulariza√ß√£o L2: {cls.L2_REGULARIZATION}")
        
        print(f"\n‚öôÔ∏è  OTIMIZA√á√ÉO:")
        print(f"   Learning Rate: {cls.LEARNING_RATE}")
        print(f"   Otimizador: {cls.OPTIMIZER}")
        
        print(f"\nüéØ TREINAMENTO:")
        print(f"   √âpocas M√°ximas: {cls.EPOCHS}")
        print(f"   Tamanho do Lote: {cls.BATCH_SIZE}")
        print(f"   Early Stopping: {cls.EARLY_STOPPING_PATIENCE} √©pocas")
        
        print(f"\nüìä DADOS:")
        print(f"   Normaliza√ß√£o: {cls.NORMALIZACAO}")
        print(f"   Teste: {cls.TEST_SIZE*100:.0f}%")
        print(f"   Valida√ß√£o: {cls.VALIDATION_SIZE*100:.0f}%")
        print(f"   CV Folds: {cls.CV_FOLDS}")
        
        print("="*70)

class RedeNeuralClassificador:
    def __init__(self, random_state=SEED):
        self.random_state = random_state
        self.model = None
        self.scaler = None
        self.history = None
        self.feature_names = None
        self.config = ConfiguracaoRedeNeural()
        
    def validar_schema(self, data):
        """Valida o schema dos dados de entrada."""
        print("Validando schema dos dados...")
        
        # Verificar se h√° 8 colunas (1 ID + 6 features + 1 target)
        if data.shape[1] != 8:
            raise ValueError(f"Esperado 8 colunas (ID + 6 features + target), encontrado {data.shape[1]}")
        
        # Remover a primeira coluna (ID) se existir
        if data.columns[0].lower() in ['id', 'index', '0'] or data.iloc[:, 0].dtype == 'int64':
            print("Removendo coluna de ID...")
            data = data.drop(data.columns[0], axis=1)
        
        # Agora devemos ter 7 colunas (6 features + 1 target)
        if data.shape[1] != 7:
            raise ValueError(f"Ap√≥s remo√ß√£o do ID, esperado 7 colunas, encontrado {data.shape[1]}")
        
        # Verificar tipos de dados das features (primeiras 6 colunas)
        for i in range(6):
            if not pd.api.types.is_numeric_dtype(data.iloc[:, i]):
                print(f"Aviso: Coluna {i+1} n√£o √© num√©rica, tentando convers√£o...")
                data.iloc[:, i] = pd.to_numeric(data.iloc[:, i], errors='coerce')
        
        # Verificar valores da classe (√∫ltima coluna)
        classes_validas = [1, 2, 3, 4]
        classes_unicas = data.iloc[:, -1].unique()
        for classe in classes_unicas:
            if classe not in classes_validas:
                raise ValueError(f"Classe inv√°lida encontrada: {classe}. Classes v√°lidas: {classes_validas}")
        
        print("‚úì Schema validado com sucesso!")
        print(f"Dados ap√≥s valida√ß√£o: {data.shape}")
        return data
    
    def tratar_valores_ausentes(self, data):
        """Trata valores ausentes ou inv√°lidos."""
        print("Tratando valores ausentes...")
        
        # Estat√≠sticas antes do tratamento
        missing_before = data.isnull().sum().sum()
        print(f"Valores ausentes encontrados: {missing_before}")
        
        if missing_before > 0:
            # Para features num√©ricas: usar mediana
            for i in range(6):
                if data.iloc[:, i].isnull().any():
                    mediana = data.iloc[:, i].median()
                    data.iloc[:, i].fillna(mediana, inplace=True)
                    print(f"Coluna {i}: preenchida com mediana = {mediana:.2f}")
            
            # Para target: remover linhas com valores ausentes
            data = data.dropna(subset=[data.columns[-1]])
        
        print("‚úì Valores ausentes tratados!")
        return data
    
    def preprocessar_dados(self, data, metodo_normalizacao=None):
        """Pr√©-processa os dados."""
        if metodo_normalizacao is None:
            metodo_normalizacao = self.config.NORMALIZACAO
            
        print(f"Iniciando pr√©-processamento (normaliza√ß√£o: {metodo_normalizacao})...")
        
        # Separar features e target
        X = data.iloc[:, :-1].values
        y = data.iloc[:, -1].values
        
        # Converter classes para √≠ndices (1,2,3,4 -> 0,1,2,3)
        y = y - 1
        
        # Normaliza√ß√£o das features
        if metodo_normalizacao == 'standard':
            self.scaler = StandardScaler()
        elif metodo_normalizacao == 'minmax':
            self.scaler = MinMaxScaler()
        else:
            raise ValueError("M√©todo deve ser 'standard' ou 'minmax'")
        
        X_scaled = self.scaler.fit_transform(X)
        
        print("‚úì Pr√©-processamento conclu√≠do!")
        return X_scaled, y
    
    def criar_modelo(self, input_dim, num_classes=4, hidden_layers=None, dropout_rate=None, l2_reg=None):
        """Cria o modelo da rede neural."""
        # Usar configura√ß√µes padr√£o se n√£o especificadas
        if hidden_layers is None:
            hidden_layers = self.config.HIDDEN_LAYERS
        if dropout_rate is None:
            dropout_rate = self.config.DROPOUT_RATE
        if l2_reg is None:
            l2_reg = self.config.L2_REGULARIZATION
            
        print(f"Criando modelo com arquitetura: {hidden_layers}")
        
        model = keras.Sequential()
        
        # Camada de entrada
        model.add(layers.Dense(hidden_layers[0], 
                              input_dim=input_dim,
                              activation='relu',
                              kernel_regularizer=keras.regularizers.l2(l2_reg)))
        model.add(layers.Dropout(dropout_rate))
        
        # Camadas ocultas
        for neurons in hidden_layers[1:]:
            model.add(layers.Dense(neurons, 
                                  activation='relu',
                                  kernel_regularizer=keras.regularizers.l2(l2_reg)))
            model.add(layers.Dropout(dropout_rate))
        
        # Camada de sa√≠da
        model.add(layers.Dense(num_classes, activation='softmax'))
        
        # Configurar otimizador
        if self.config.OPTIMIZER == 'adam':
            optimizer = keras.optimizers.Adam(learning_rate=self.config.LEARNING_RATE)
        elif self.config.OPTIMIZER == 'rmsprop':
            optimizer = keras.optimizers.RMSprop(learning_rate=self.config.LEARNING_RATE)
        elif self.config.OPTIMIZER == 'sgd':
            optimizer = keras.optimizers.SGD(learning_rate=self.config.LEARNING_RATE, momentum=0.9)
        else:
            raise ValueError("Otimizador deve ser 'adam', 'rmsprop' ou 'sgd'")
        
        # Compilar modelo
        model.compile(
            optimizer=optimizer,
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
        
        print("‚úì Modelo criado e compilado!")
        return model
    
    def treinar_modelo(self, X_train, y_train, X_val, y_val, epochs=None, batch_size=None):
        """Treina o modelo com early stopping e redu√ß√£o de learning rate."""
        if epochs is None:
            epochs = self.config.EPOCHS
        if batch_size is None:
            batch_size = self.config.BATCH_SIZE
            
        print("Iniciando treinamento...")
        
        # Callbacks
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=self.config.EARLY_STOPPING_PATIENCE,
            restore_best_weights=True,
            verbose=1
        )
        
        reduce_lr = ReduceLROnPlateau(
            monitor='val_loss',
            factor=self.config.REDUCE_LR_FACTOR,
            patience=self.config.REDUCE_LR_PATIENCE,
            min_lr=self.config.MIN_LEARNING_RATE,
            verbose=1
        )
        
        # Treinamento
        self.history = self.model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=(X_val, y_val),
            callbacks=[early_stopping, reduce_lr],
            verbose=1
        )
        
        print("‚úì Treinamento conclu√≠do!")
        return self.history
    
    def validacao_cruzada(self, X, y, k=None):
        """Realiza valida√ß√£o cruzada estratificada."""
        if k is None:
            k = self.config.CV_FOLDS
            
        print(f"Iniciando valida√ß√£o cruzada (k={k})...")
        
        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=self.random_state)
        scores = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            print(f"Fold {fold + 1}/{k}")
            
            X_train_fold, X_val_fold = X[train_idx], X[val_idx]
            y_train_fold, y_val_fold = y[train_idx], y[val_idx]
            
            # Criar novo modelo para cada fold
            modelo_fold = self.criar_modelo(X.shape[1])
            
            # Treinar com menos √©pocas para CV
            modelo_fold.fit(
                X_train_fold, y_train_fold,
                epochs=self.config.CV_EPOCHS,
                batch_size=self.config.BATCH_SIZE,
                validation_data=(X_val_fold, y_val_fold),
                verbose=0
            )
            
            # Avaliar
            score = modelo_fold.evaluate(X_val_fold, y_val_fold, verbose=0)[1]
            scores.append(score)
            print(f"Acur√°cia Fold {fold + 1}: {score:.4f}")
        
        print(f"‚úì Valida√ß√£o cruzada conclu√≠da!")
        print(f"Acur√°cia m√©dia: {np.mean(scores):.4f} (¬±{np.std(scores):.4f})")
        return scores
    
    def calcular_importancia_features(self, X_test, y_test):
        """Calcula a import√¢ncia das features usando permutation importance manual."""
        print("Calculando import√¢ncia das features...")
        
        # Acur√°cia base (sem permuta√ß√£o)
        y_pred_base = np.argmax(self.model.predict(X_test, verbose=0), axis=1)
        acc_base = accuracy_score(y_test, y_pred_base)
        print(f"Acur√°cia base: {acc_base:.4f}")
        
        # Calcular import√¢ncia para cada feature
        n_features = X_test.shape[1]
        importancias = []
        importancias_std = []
        
        for i in range(n_features):
            print(f"Processando feature {i+1}/{n_features}...")
            
            # Lista para armazenar as diferen√ßas de acur√°cia
            diferencas = []
            
            # Repetir v√°rias vezes para ter estat√≠stica mais robusta
            for rep in range(self.config.PERMUTATION_REPEATS):
                # Copiar os dados
                X_perm = X_test.copy()
                
                # Permutar a feature i
                np.random.seed(self.random_state + rep)
                X_perm[:, i] = np.random.permutation(X_perm[:, i])
                
                # Calcular nova acur√°cia
                y_pred_perm = np.argmax(self.model.predict(X_perm, verbose=0), axis=1)
                acc_perm = accuracy_score(y_test, y_pred_perm)
                
                # Diferen√ßa (import√¢ncia = queda na acur√°cia)
                diferencas.append(acc_base - acc_perm)
            
            # Estat√≠sticas da import√¢ncia
            media_imp = np.mean(diferencas)
            std_imp = np.std(diferencas)
            
            importancias.append(media_imp)
            importancias_std.append(std_imp)
            
            print(f"Feature {i+1}: Import√¢ncia = {media_imp:.4f} (¬±{std_imp:.4f})")
        
        # Criar objeto similar ao sklearn
        class PermutationImportanceResult:
            def __init__(self, importances_mean, importances_std):
                self.importances_mean = np.array(importances_mean)
                self.importances_std = np.array(importances_std)
        
        print("‚úì C√°lculo de import√¢ncia conclu√≠do!")
        return PermutationImportanceResult(importancias, importancias_std)
    
    def extrair_embeddings(self, X):
        """Extrai embeddings da pen√∫ltima camada."""
        print("Extraindo embeddings...")
        
        # Primeiro, fazer uma predi√ß√£o dummy para construir o modelo
        _ = self.model.predict(X[:1], verbose=0)
        
        # Agora podemos criar o extrator de features da pen√∫ltima camada
        try:
            # Tentar acessar a pen√∫ltima camada (antes da softmax)
            extrator_features = keras.Model(inputs=self.model.input,
                                           outputs=self.model.layers[-2].output)
        except:
            # Se n√£o conseguir, usar uma abordagem alternativa
            print("Usando abordagem alternativa para extrair embeddings...")
            # Criar um modelo tempor√°rio sem a √∫ltima camada
            temp_model = keras.Sequential()
            for layer in self.model.layers[:-1]:  # Todas exceto a √∫ltima
                temp_model.add(layer)
            
            # Fazer uma predi√ß√£o para construir o modelo tempor√°rio
            _ = temp_model.predict(X[:1], verbose=0)
            extrator_features = temp_model
        
        # Extrair embeddings
        embeddings = extrator_features.predict(X, verbose=0)
        
        return embeddings
    
    def teste_robustez(self, X_test, y_test, niveis_ruido=None):
        """Testa robustez adicionando ru√≠do gaussiano."""
        if niveis_ruido is None:
            niveis_ruido = self.config.NOISE_LEVELS
            
        print("Testando robustez com ru√≠do gaussiano...")
        
        resultados = []
        
        # Teste sem ru√≠do
        y_pred_sem_ruido = np.argmax(self.model.predict(X_test, verbose=0), axis=1)
        acc_sem_ruido = accuracy_score(y_test, y_pred_sem_ruido)
        resultados.append(('Sem ru√≠do', 0.0, acc_sem_ruido))
        print(f"Acur√°cia sem ru√≠do: {acc_sem_ruido:.4f}")
        
        # Testes com ru√≠do
        for nivel in niveis_ruido:
            ruido = np.random.normal(0, nivel, X_test.shape)
            X_test_ruido = X_test + ruido
            
            y_pred_ruido = np.argmax(self.model.predict(X_test_ruido, verbose=0), axis=1)
            acc_ruido = accuracy_score(y_test, y_pred_ruido)
            resultados.append((f'Ru√≠do œÉ={nivel}', nivel, acc_ruido))
            print(f"Acur√°cia com ru√≠do œÉ={nivel}: {acc_ruido:.4f}")
        
        return resultados
    
    def executar_treinamento(self, data_path, feature_names=None):
        """Executa apenas o treinamento e avalia√ß√£o b√°sica."""
        import time
        tempo_inicio = time.time()
        
        print("="*60)
        print("INICIANDO TREINAMENTO DA REDE NEURAL")
        print("="*60)
        
        # Imprimir configura√ß√µes atuais
        self.config.imprimir_configuracoes()
        
        # 1. Carregar e validar dados
        print("\n1. CARREGAMENTO E VALIDA√á√ÉO DOS DADOS")
        print("-"*40)
        
        data = pd.read_csv(data_path, header=None)
        print(f"Dados carregados: {data.shape}")
        
        # Nomear as colunas
        data.columns = ['ID', 'Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5', 'Feature6', 'Target']
        
        data = self.validar_schema(data)
        data = self.tratar_valores_ausentes(data)
        
        if feature_names is None:
            feature_names = ['Sinal Vital 1', 'Sinal Vital 2', 'Sinal Vital 3', 'Sinal Vital 4', 'Sinal Vital 5', 'Sinal Vital 6']
        self.feature_names = feature_names
        
        # 2. An√°lise explorat√≥ria b√°sica
        print("\n1.1. AN√ÅLISE EXPLORAT√ìRIA")
        print("-"*40)
        print("Distribui√ß√£o das classes:")
        print(data.iloc[:, -1].value_counts().sort_index())
        
        # 3. Pr√©-processamento
        print("\n2. PR√â-PROCESSAMENTO")
        print("-"*40)
        X, y = self.preprocessar_dados(data)
        
        # 4. Divis√£o estratificada
        print("\n3. DIVIS√ÉO DOS DADOS")
        print("-"*40)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=self.config.TEST_SIZE, stratify=y, random_state=self.random_state
        )
        
        X_train_split, X_val, y_train_split, y_val = train_test_split(
            X_train, y_train, test_size=self.config.VALIDATION_SIZE, stratify=y_train, random_state=self.random_state
        )
        
        print(f"Treino: {X_train_split.shape[0]} amostras")
        print(f"Valida√ß√£o: {X_val.shape[0]} amostras")
        print(f"Teste: {X_test.shape[0]} amostras")
        
        # 5. Criar e treinar modelo
        print("\n4. CRIA√á√ÉO E TREINAMENTO DO MODELO")
        print("-"*40)
        tempo_treinamento_inicio = time.time()
        
        self.model = self.criar_modelo(X.shape[1])
        self.treinar_modelo(X_train_split, y_train_split, X_val, y_val)
        
        tempo_treinamento = time.time() - tempo_treinamento_inicio
        print(f"‚è±Ô∏è  Tempo de treinamento: {tempo_treinamento:.2f}s")
        
        # 6. Valida√ß√£o cruzada
        print("\n5. VALIDA√á√ÉO CRUZADA")
        print("-"*40)
        cv_scores = self.validacao_cruzada(X_train, y_train)
        
        # 7. Avalia√ß√£o final
        print("\n6. AVALIA√á√ÉO FINAL")
        print("-"*40)
        y_pred = np.argmax(self.model.predict(X_test, verbose=0), axis=1)
        
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        print(f"Acur√°cia: {accuracy:.4f}")
        print(f"F1-Score: {f1:.4f}")
        print("\nRelat√≥rio de Classifica√ß√£o:")
        print(classification_report(y_test, y_pred, 
                                  target_names=['Classe 1', 'Classe 2', 'Classe 3', 'Classe 4']))
        
        tempo_total = time.time() - tempo_inicio
        print(f"\n‚è±Ô∏è  Tempo total: {tempo_total:.2f}s ({tempo_total/60:.1f} min)")
        
        return {
            'X_train': X_train_split,
            'X_val': X_val,
            'X_test': X_test,
            'y_train': y_train_split,
            'y_val': y_val,
            'y_test': y_test,
            'y_pred': y_pred,
            'accuracy': accuracy,
            'f1_score': f1,
            'cv_scores': cv_scores,
            'tempo_total': tempo_total
        }